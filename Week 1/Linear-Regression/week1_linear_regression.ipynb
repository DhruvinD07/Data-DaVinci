{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR REGRESSION\n",
    "In this assignment we try to model the 'Estimated Price' as a linear relation of the other elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSES IN PYTHON\n",
    "Although this might look scary to implement, go about it one function at a time.  \n",
    "Using classes help with keeping track of multiple models and makes your overall code much tidier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self) -> None:\n",
    "        '''\n",
    "        Constructor for the class\n",
    "        Defines member variables weights and bias and initializes them to zero\n",
    "        They can be accessed in other functions of the class by using self.weights and self.bias\n",
    "        '''\n",
    "        self.weights: np.ndarray | None = None\n",
    "        self.bias: float | None = None\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Computes the predicted values based on calculated weights and bias given an input array\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "        \n",
    "        Returns:\n",
    "            An array of predicted values Y_pred (np.ndarray)\n",
    "        '''\n",
    "        return X @ self.weights + self.bias\n",
    "\n",
    "    def __loss(self, X: np.ndarray, y: np.ndarray, norm: int) -> tuple:\n",
    "        '''\n",
    "        The loss function is a variant of Minkowski distance.\n",
    "        Minkowski distance between X and Y is defined as:\n",
    "            (1 / n) * [sum(abs(xi - yi) ^ p)]\n",
    "        Return the loss, and gradients with respect to the weights and bias\n",
    "        \n",
    "        Args:\n",
    "            X_input (np.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "            Y_expected (np.ndarray): Target vector of shape (n_samples,).\n",
    "            norm (int): Order of Minkowski distance.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: loss, dw, db\n",
    "        '''\n",
    "        \n",
    "        length=len(X)\n",
    "        var=self.predict(X)-y\n",
    "       \n",
    "        loss=np.sum(np.abs(var)**norm) / length\n",
    "        a=norm * (var**(norm-1)) / length #derivative of each individual term \n",
    "        dw=np.dot((X.T),a)\n",
    "        db=np.sum(a)\n",
    "       \n",
    "\n",
    "        return loss, dw, db\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 500, learning_rate: float = 0.01, norm: int = 2, threshold: float = 0.0001) -> None:\n",
    "        '''\n",
    "        Applies gradient descent on the given data and tunes the values of the weights and bias\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "            y (np.ndarray): Target vector of shape (n_samples,).\n",
    "            epochs (int): Maximum number of iterations of gradient descent.\n",
    "            learning_rate (float): Constant which alters the rate of convergence\n",
    "            norm (int): Norm for the loss function\n",
    "            threshold (float): Stops the gradient descent if the change in loss or cost is below this value\n",
    "            lambda_reg (float): Regularization constant (Apply L1 regularization)\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        # Initialize weights randomly and set up gradient descent\n",
    "        self.weights = np.random.rand(len(X[0]))\n",
    "        self.bias = np.random.rand()\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            # Logic for gradient descent (use the loss function to get gradients)\n",
    "            p=self.__loss(X, y, norm)\n",
    "            self.weights = self.weights - learning_rate*p[1]\n",
    "            self.bias = self.bias - learning_rate*p[2]\n",
    "            if abs(p[0]-self.__loss(X, y, norm)[0])<threshold:\n",
    "                break\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Converting Data\n",
    "Some features in a dataset are not of numerical type and are either categorical or boolean.  \n",
    "To get past this, we convert the columns by using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "df = pd.read_csv('linear_data.csv')\n",
    "\n",
    "### TODO 4\n",
    "# convert columns from categorical/boolean to integer (use one-hot encoding)\n",
    "df['Manual Transmission'] = df['Manual Transmission'].astype(int)   # Converting the 'Manual Transmission' column from boolean to 0/1     pd.get_dummies(df, columns = ['Manual Transmission', 'Fuel'], dtype = int, drop_first=True) is also a method to do this, found this out much later\n",
    "df= pd.get_dummies(df, columns=['Fuel'])                         # One-hot encoding the 'Fuel' column\n",
    "\n",
    "fuel_columns = [col for col in df.columns if col.startswith('Fuel')]   # Getting the columns of one-hot encoded 'Fuel' column\n",
    "df[fuel_columns] = df[fuel_columns].astype(int)     # Converting the one-hot encoded columns to int\n",
    "\n",
    "#print(df.head())\n",
    "X, y = df.drop('Estimated Price', axis = 1).to_numpy(), df['Estimated Price'].to_numpy() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-train split\n",
    "Overfitting is one of the biggest problems in machine learning. Overfitting occurs when the model is trained to be very accurate on the given dataset but performs very poorly on a different but similar dataset.\n",
    "To check for overfitting, we split our dataset into test and train sets and check the accuracy/loss of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Standardization\n",
    "Since some features might have much higher values than the others, for weights of similar magnitude, the model will mainly focus only on features with large values.  \n",
    "To overcome this, we standardize each feature using Z-Score Standardization so that all features are treated equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score standardization\n",
    "### TODO 5\n",
    "def z_score(X: np.ndarray) -> tuple:\n",
    "    '''\n",
    "    The Z-Score scales data such that its mean is 0 and standard deviation is 1\n",
    "    z-score for a value x in the dataset is (x - mean) / std_dev\n",
    "    (z-score normalization is done over a feature and NOT an entry)\n",
    "    Return the z-score value of all the elements in the set along with the mean and standard deviation of the original set\n",
    "    '''\n",
    "\n",
    "    # Add code here\n",
    "    x_mean = np.mean(X, axis = 0)\n",
    "    x_std = np.std(X, axis = 0)\n",
    "    x = (X - x_mean) / x_std\n",
    "    return x, x_mean, x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "x_train, x_mean, x_std = z_score(X_train)\n",
    "x_test = (X_test - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train, epochs=, learning_rate=, norm=, threshold=)\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"MSE loss: \", np.mean((y_pred - y_test) ** 2))\n",
    "\n",
    "indices = np.arange(len(y_test))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(indices, y_test, label='True Values', color='blue', marker='o')\n",
    "plt.plot(indices, y_pred, label='Predicted Values', color='red', marker='x')\n",
    "\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Values')\n",
    "plt.title('True vs Predicted Values')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
