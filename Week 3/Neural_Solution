import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, InputLayer
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from sklearn.model_selection import train_test_split
import numpy as np

# Path to dataset
dataset_path = "/Users/dhruvin/Documents/Firsttimepython/Week3_Homer&Bart"

# Load all images at once
datagen = ImageDataGenerator(rescale=1.0/255.0)  # Normalize pixel values

data = next(datagen.flow_from_directory(
    directory=dataset_path,
    target_size=(256, 256),  # Larger image size
    batch_size=5000,  # Load all images at once
    class_mode="categorical"  # One-hot encoding
))

# Split dataset into training and testing
X_train, X_test, Y_train, Y_test = train_test_split(data[0], data[1], test_size=0.2, random_state=42)

# Define model (SIMPLER version like your friend’s)
model = Sequential([
    InputLayer(input_shape=(256, 256, 3)),  # Input layer
    Flatten(),
    Dense(64, activation='relu'),  # Fewer neurons (like friend’s model)
    Dense(16, activation='relu'),
    Dense(2, activation='softmax')  # Softmax for categorical labels
])

# Compile model (Adam optimizer with slightly higher learning rate)
model.compile(optimizer=Adam(learning_rate=0.001), loss=CategoricalCrossentropy(), metrics=['accuracy'])

# Train model (LARGER batch size = more stable training)
model.fit(X_train, Y_train, epochs=10, batch_size=64, validation_data=(X_test, Y_test))

# Evaluate model
test_loss, test_acc = model.evaluate(X_test, Y_test)
print(f'Test accuracy: {test_acc}')

